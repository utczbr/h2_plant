Architectural Performance Audit and High-Performance Optimization Roadmap for Hydrogen Plant Simulation (h2_plant)Executive SummaryThis report delivers an exhaustive architectural audit and performance optimization strategy for the h2_plant Python-based Hydrogen Plant Simulator. The analysis is conducted from the perspective of high-performance computing (HPC) software architecture, specifically targeting the elimination of interpretation overhead, memory latency, and serialization costs inherent in mixed-mode (Python + Numba) scientific applications. The current system employs hybrid optimizations—Lookup Table (LUT) Managers to bypass CoolProp latency and Just-In-Time (JIT) compilation via numba_ops.py for thermodynamic calculations. While these measures address the computational intensity of thermodynamic state equations, they fail to address the structural bottlenecks imposed by the Python object model.The requirement to support Monte Carlo simulations and long-term optimization runs (8760h+) necessitates a fundamental shift from "optimized Python" to a "Data-Oriented Design" (DoD) paradigm. Our analysis identifies that the primary bottlenecks are no longer the mathematical computations, which have been successfully offloaded to LUTs and JIT kernels. Instead, the bottlenecks have shifted to data marshalling, memory access patterns, and dynamic dispatch overhead. The usage of Stream dataclasses and dictionary-based flow network execution introduces pointer indirection and cache coherency issues that prevent modern CPUs from utilizing their vector units (SIMD) and prefetchers effectively.This report proposes a three-tiered optimization plan:Level 1: Code Optimizations – Micro-optimizations to reduce overhead in the current object-oriented structure, focusing on method pre-resolution and LUT memory layout.Level 2: Data Structure Refactoring – A transition to localized memory structures to reduce Garbage Collection (GC) pressure and improve cache locality, utilizing static graph linearization.Level 3: Architectural Transformation – A complete migration to a Structure of Arrays (SoA) layout and static "Mega-Kernels," enabling fully vectorized timesteps and zero-copy data flow.The ultimate conclusion indicates that memory latency due to pointer chasing (Object-Oriented Design) is the single largest bottleneck remaining. Eliminating this through Data-Oriented Design is the only path to the order-of-magnitude speedups required for Monte Carlo campaigns.1. Theoretical Foundation of Performance in Python SimulationsTo rigorously analyze the performance characteristics of the h2_plant simulator, one must first establish the theoretical constraints of the Python interpreter and modern hardware architecture. The "Python tax" in scientific computing is often misunderstood as simply "slow loops," but in high-frequency simulation, it manifests primarily as memory inefficiency, dispatch latency, and the inability to exploit modern processor features.1.1 The Interpreter Overhead and Boxed ObjectsPython is a dynamically typed language where every variable is a heap-allocated object (PyObject). A simple floating-point number in Python is not 64 bits of raw memory; it is a complex structure containing a reference count, a type pointer, and the value itself. This "boxing" of data is the antithesis of high-performance computing.In the context of the h2_plant simulation, the Stream class acts as the primary data carrier. When a Stream object holds a temperature value, accessing that value involves a convoluted sequence of operations. First, the interpreter must dereference the pointer to the Stream object itself. Next, it must look up the attribute name (e.g., 'T') in the object's dictionary (__dict__) or slot table. Finally, it must dereference the resulting float object and unbox the raw double value to perform any computation. This process occurs for every single property access.Research into Python's data model indicates that while dataclasses provide syntactic sugar and some boilerplate reduction, they do not inherently solve the memory overhead problem. Snippets 1 highlight that dataclass attribute access, while faster than standard class dictionary lookups, still incurs significant overhead compared to direct array access or C-struct access. In a simulation requiring millions of state updates per second (8760 hours $\times$ sub-hourly timesteps $\times$ thousands of Monte Carlo iterations), these pointer dereferences accumulate to form a massive wall of latency. The initialization overhead of these objects also plays a critical role; if Stream objects are created or destroyed within the loop, the Garbage Collector (GC) becomes a dominant factor in execution time.11.2 The Von Neumann Bottleneck and Data LocalityModern high-performance computing is constrained less by CPU clock speed and more by memory bandwidth—the "Von Neumann Bottleneck." Processors have become significantly faster than the memory subsystems feeding them. To mitigate this discrepancy, hardware relies on a hierarchy of caches (L1, L2, L3). The efficiency of these caches is the single most important determinant of simulation speed.Cache efficiency relies heavily on spatial locality. When a processor fetches a byte from memory, it typically fetches an entire cache line (usually 64 bytes). If the data structures are contiguous in memory—such as in a C array or a NumPy array—fetching index i essentially pre-loads indices i+1, i+2, and so on into the high-speed cache. This allows the CPU to process subsequent elements without stalling.The current implementation of h2_plant, relying on lists of Stream objects, creates a "scattered" memory pattern. Each Stream object is allocated independently on the heap at an arbitrary memory address. Iterating over a list of streams implies jumping randomly through the computer's RAM. This phenomenon, known as "pointer chasing," effectively neutralizes the benefits of Numba's JIT compilation. Even if the mathematical instructions are optimized, the CPU spends the vast majority of its time waiting for data to be fetched from main memory, resulting in a high rate of cache misses.4 This architectural flaw prevents the hardware prefetcher from anticipating the next data point, leading to pipeline stalls.1.3 The Numba Marshalling TaxThe project utilizes numba_ops.py for JIT-compiled thermodynamic calculations. While Numba is a powerful tool for optimizing numerical functions, its integration into a Python-heavy codebase is not without cost. Numba functions are compiled to machine code via LLVM, but the boundary between the Python interpreter and the compiled function represents a significant "marshalling" barrier.When a list of Python objects (like Stream instances) is passed to a Numba-compiled function, Numba must bridge the gap between Python's dynamic type system and the static types required by the compiled code. This often involves:Unboxing: Extracting the raw data pointers from the Python objects.Type Checking: Verifying that the objects match the expected signature.Iteration: Looping through the list to extract values, often triggering the Python API.Re-boxing: Packaging the results back into Python objects upon return.If the h2_plant engine calls a JIT-compiled thermodynamic function inside a loop—for example, once per component per timestep—this marshalling cost is paid per iteration. Research snippet 6 and  explicitly warn that argument parsing and unboxing can double the runtime for simple functions. In many cases, the overhead of calling the function exceeds the time taken to execute the function's body. This "call overhead" effectively negates the advantage of JIT compilation when the granularity of the work is small.1.4 Vectorization and Instruction Level ParallelismModern CPUs are equipped with SIMD (Single Instruction, Multiple Data) units, such as AVX-512, which can perform mathematical operations on multiple data points simultaneously. For example, a single instruction can multiply 8 double-precision floating-point numbers at once. To utilize this capability, data must be arranged in contiguous blocks (vectors), and the operations must be uniform.The current object-oriented design, characterized by heterogeneous lists of components and individual method calls (e.g., component.update()), presents a barrier to vectorization. The Python interpreter cannot automatically batch these polymorphic calls into vector instructions. Each call is processed sequentially, creating a dependency chain that prevents the CPU from exploiting instruction-level parallelism. Snippets 5 emphasize that "Structure of Arrays" (SoA) layouts are essential for leveraging these hardware capabilities, whereas the "Array of Structures" (AoS) layout used in h2_plant forces scalar execution.2. Critical Architectural Audit of h2_plantThe following audit provides a detailed dissection of the provided modules, mapping the theoretical bottlenecks identified above to specific lines of code and architectural patterns within the simulator.2.1 The Object-Oriented Trap: stream.py and component_registry.pyThe stream.py and component_registry.py modules enforce an Object-Oriented Programming (OOP) structure where the plant is modeled as a graph of connected objects. While this semantic mapping is intuitive for engineering conceptualization, it is catastrophic for performance in high-throughput Monte Carlo simulations.The Bottleneck:The Stream class, presumably implemented as a dataclass, holds the thermodynamic state (Temperature, Pressure, Enthalpy, Mass Flow). The flow_network.py manages connections between these streams.Memory Overhead: A simulation with hundreds of streams involves hundreds of separate heap allocations. As snippet  suggests, the overhead of creating and managing these objects is non-negligible.Garbage Collection Pressure: If the simulation logic involves creating new Stream objects or temporary state carriers during timesteps, it triggers Python's reference counting and cyclic garbage collector. This results in unpredictable "stop-the-world" pauses.Attribute Lookup Latency: Every time the engine reads stream.T, it incurs the interpreter dispatch cost. As noted in 2, attribute access in dataclasses is slower than direct dictionary access and significantly slower than array indexing.Evidence:Research snippet 3 benchmarks dataclass instantiation against other methods, showing that while they are faster than some high-level frameworks like Pydantic, they are orders of magnitude slower than native operations. Snippet 9 discusses the performance penalty of frozen dataclasses, creating a tension between safety and speed. Ultimately, the use of individual objects for what is essentially a vector of numbers is a fundamental mismatch for the simulation's performance goals.2.2 Lookup Table (LUT) Strategy: lut_manager.py and coolprop_lut.pyThe implementation of lut_manager.py and coolprop_lut.py uses interpolation to bypass the latency of CoolProp library calls. This is a sound algorithmic optimization, reducing the cost of state evaluation from milliseconds to microseconds. However, the implementation likely suffers from cache inefficiency and sub-optimal memory access patterns.The Bottleneck:Interpolation Algorithm: Standard bilinear interpolation involves identifying the four nearest neighbors in a grid and computing a weighted average. If the LUT is stored as a 2D NumPy array, the memory access pattern is dictated by the array's layout.Cache Thrashing: If the interpolation function is called for random (Temperature, Pressure) pairs across the operating envelope—common in dynamic simulations—the CPU must fetch widely separated parts of the LUT array. Snippets 10 and 11 highlight the importance of memory layout. If the array is stored in C-order (row-major), accessing table[i, j] and table[i, j+1] implies a stride across memory that may exceed the cache line size, depending on the row width. Conversely, Fortran-order (column-major) has different locality characteristics. The mismatch between the access pattern of the interpolation algorithm and the physical layout of the array leads to frequent cache misses.Branching Overhead: The search for the correct grid indices (often using np.searchsorted or bisect) introduces conditional branching inside the inner loop. Modern CPUs rely on branch prediction pipelines; random or data-dependent branching flushes these pipelines, wasting cycles.Evidence:Snippet 12 provides an efficient bilinear interpolation implementation that minimizes loops, but the memory layout remains a critical factor. Snippet 11 explicitly states that Fortran-style order is faster for column-wise access, while C-style is faster for row-wise. Without aligning the data structure to the access pattern of the interpolation kernel, the simulator leaves significant performance on the table.2.3 Engine & Dispatch: engine.py and engine_dispatch.pyThe engine.py main loop iterates over a heterogeneous list of components (Electrolyzers, Compressors, Tanks) and invokes a method (e.g., solve() or update()) on each.The Bottleneck:Polymorphism preventing Optimization: The call component.update() is polymorphic. The interpreter must inspect the type of component at runtime to determine which code to execute. This dynamic dispatch prevents instruction pipelining and optimization.Vectorization Barrier: It is impossible to vectorize a loop that calls different functions on different data types. CPU vector units operate on uniform arrays of data (e.g., "multiply these 16 floats by those 16 floats"). They cannot "update this Electrolyzer, then update that Tank."The Interpretation Loop: The loop over components runs in the Python interpreter. Even if the individual component methods are JIT-compiled via Numba, the loop itself is not. Control returns to the slow interpreter after every single component update, incurring the dispatch and marshalling costs repeatedly.Evidence:Research snippet 13 clarifies that Numba code compiled in object mode (which happens when interacting with Python objects inside loops) is barely faster than the interpreter. To achieve "native" speed, the loop must be lifted entirely into nopython mode, which the current heterogeneous object structure prohibits.2.4 Flow Network: flow_network.py and flow_tracker.pyThe execute_flows method in flow_network.py performs too many dictionary lookups per timestep.The Bottleneck:Graph Traversal: If the flow execution involves traversing a graph (Component A $\rightarrow$ Stream 1 $\rightarrow$ Component B) using a dictionary adjacency list, it performs hash table lookups in the critical path.Hash Overhead: While dictionary lookups are theoretically $O(1)$, the constant factor involves hashing the key, handling collisions, and probing the memory bucket. Compared to an array index lookup, which is a single machine instruction, dictionary lookups are computationally expensive.Dynamic Resolution: The topology of the plant (pipes and connections) is static; it does not change during a standard simulation run. Resolving the flow order dynamically at every timestep is redundant computation.Evidence:Snippet 14 explicitly states that local variable lookups are faster than global or dictionary lookups. Snippet 15 and 16 reinforce that array indexing is vastly superior to dictionary lookups for performance-critical loops. The current implementation pays a high price for the flexibility of dictionaries where it is not needed.2.5 Numba Integration: numba_ops.pyThe numba_ops.py module contains JIT-compiled functions. The critical question is how data flows into and out of these functions.The Bottleneck:If the engine passes a Stream object to a Numba function, Numba treats it as a generic pyobject unless it is a specifically typed jitclass or StructRef. As noted in 17, accessing attributes of a generic object inside a JIT function requires interacting with the Python C-API, forcing Numba to fall back to "Object Mode." This effectively disables the optimizations Numba is designed to provide. Even if passing NumPy arrays, if those arrays are created dynamically inside the loop (e.g., np.array()), the memory allocation cost dominates the execution time.3. Performance Optimization PlanThe proposed optimization plan is structured into three levels of increasing complexity and impact. Each level addresses specific bottlenecks identified in the audit.3.1 Level 1: Code Optimizations (Low Hanging Fruit)These optimizations focus on reducing overhead within the existing architectural framework. They involve specific changes to loops, data types, and pre-calculations that do not require a complete rewrite of the simulation logic.3.1.1 Pre-Resolution of Attributes and MethodsThe Problem: In the main loop of engine.py, repeated access to methods like component.update or lut_manager.get_property forces the interpreter to perform attribute lookups in every iteration.The Solution: Cache method references in local variables before entering the critical loop.Python# Optimization Pattern
# Before:
for t in time_steps:
    lut_manager.get_enthalpy(T, P)

# After:
get_enthalpy = lut_manager.get_enthalpy
for t in time_steps:
    get_enthalpy(T, P)
Expected Gain: Micro-optimization that removes the dictionary lookup overhead for method resolution per iteration. In tight loops, this can yield measurable improvements.14Accuracy Risk: None.3.1.2 Homogenization of Floating Point TypesThe Problem: Mixing Python native floats (which are double precision) with NumPy float32 or float64 types can trigger implicit casting and temporary object creation.The Solution: Enforce float64 (double precision) uniformly across the application. Configure Numba signatures to expect this specific type. This avoids the overhead of type checking and conversion at the boundary of JIT functions.Expected Gain: Elimination of casting overhead and improved consistency for the JIT compiler.Accuracy Risk: float64 is standard for scientific computing. Using float32 might introduce precision errors >0.1%, so float64 is the safe and performant choice for thermodynamics.3.1.3 LUT Memory Layout OptimizationThe Problem: The LUT data is accessed via bilinear interpolation, which queries neighbors in a grid. If the memory layout does not match the access pattern, cache misses occur.The Solution:C-Contiguous Arrays: Ensure all LUT NumPy arrays are stored in C-contiguous order (order='C').Row-Major Access: Write the interpolation kernel to access the table row by row. This exploits the spatial locality of C-ordered arrays, where elements in the same row are adjacent in memory.Kernel Optimization: Use numba.njit with boundscheck=False for the interpolation kernel to remove the overhead of boundary checking logic, assuming inputs are validated at the entry point.Evidence: Snippet 11 confirms that matching the access pattern to the memory layout (row-wise for C-order) significantly reduces cache misses.Expected Gain: 20-30% speedup in thermodynamic property lookups due to improved L1 cache hit rates.3.2 Level 2: Data Structure Refactoring (Object Overhead Reduction)This level addresses the structural inefficiencies of the Stream class and ComponentRegistry by refactoring data structures to be more memory-efficient and cache-friendly.3.2.1 Optimizing Stream with __slots__ or StructRefThe Problem: Stream objects using __dict__ consume excessive memory and have slow access times. Passing them to Numba is inefficient.The Solution:Intermediate Step (__slots__): Define __slots__ for the Stream class. This removes the dynamic dictionary for each instance, reducing memory footprint and speeding up attribute access.2Advanced Step (StructRef): Utilize Numba's StructRef (experimental) feature. This allows the definition of a mutable, struct-like object that Numba can natively understand and manipulate without the unboxing overhead of pure Python objects. Snippet 18 and 19 indicate that StructRef is preferred over jitclass for cache support and performance in complex scenarios.Expected Gain: Significant reduction in marshalling costs. Stream objects can be passed directly into JIT-compiled flow solvers without falling back to object mode.3.2.2 Static Topology Resolution (Graph Linearization)The Problem: execute_flows dynamically traverses the graph at every timestep, wasting cycles on static logic.The Solution: Implement Static Graph Compilation. Since the plant topology is immutable during a run, we can "compile" the flow execution order into a linear sequence.Topological Sort: Perform a topological sort of the flow network during initialization.20Linear List: Generate a flat list of components in the order they must be solved to satisfy dependencies.Simplified Loop: The main loop simply iterates through this pre-computed list: for component in execution_list: component.solve().Expected Gain: Elimination of all graph traversal logic and dictionary lookups during the timestep loop. Transformation of the flow solver into a linear iteration over a list.Accuracy Risk: None, provided the graph logic handles algebraic loops correctly (e.g., via tearing or iterative solvers) during the sort phase.3.2.3 Zero-Copy Data Transfer for NumbaThe Problem: Passing lists of Stream objects forces Numba to iterate and unbox.The Solution: Refactor the data passing mechanism to use NumPy Arrays. Instead of passing objects, pass the underlying data arrays.Global State Array: Create a large (N_streams, N_properties) NumPy array that holds the state of all streams.View-Based Objects: Refactor Stream objects to hold views or indices into this global array rather than owning the data.Direct Access: Numba functions receive the GlobalStateArray and an index i, allowing them to operate directly on native memory with zero copy.Evidence: Snippet 22 in the context of GPU/Numba highlights the importance of zero-copy transfer. The same principle applies to CPU JIT boundaries: passing a pointer to an array is infinitely faster than serializing a list of objects.3.3 Level 3: Architectural Changes (Pure HPC / Data-Oriented Design)This level proposes a radical transformation of the engine's architecture to achieve the performance required for massive Monte Carlo runs. It involves abandoning the "Array of Objects" pattern in favor of "Structure of Arrays" (SoA).3.3.1 Migration to Structure of Arrays (SoA)The Problem: The current "Array of Objects" (list of Stream instances) causes pointer chasing and prevents SIMD vectorization.The Solution: Refactor the entire engine state into a monolithic State Matrix or a Structure of Arrays container.Implementation Strategy:Instead of:Python# Array of Structures (AoS) - Slow
class Stream:
    T: float
    P: float
    mdot: float

streams =
Use Data-Oriented Design (SoA):Python# Structure of Arrays (SoA) - Fast
class PlantState:
    def __init__(self, n_streams):
        self.T = np.zeros(n_streams, dtype=np.float64)
        self.P = np.zeros(n_streams, dtype=np.float64)
        self.mdot = np.zeros(n_streams, dtype=np.float64)
Why this is faster:SIMD Vectorization: We can update the temperature of all streams in a single vector operation. state.H = get_enthalpy(state.T, state.P). This allows Numba to emit AVX-512 instructions that process 8 or 16 streams simultaneously per CPU cycle.4Cache Locality: The T array is a contiguous block of memory. When the CPU loads T, it loads T...T into the cache line automatically. The hardware prefetcher can predict the access pattern perfectly.No Marshalling: The entire state is just a few NumPy arrays, which are trivial to pass to Numba.Evidence:Snippet 8 clearly articulates the SoA vs. AoS trade-off. While SoA can be less intuitive for programmers used to OOP, it is the standard for high-performance physics simulations. Snippet 25 demonstrates that Numba combined with optimized NumPy layouts can rival C++ and Fortran performance.3.3.2 The Static Graph Compilation Strategy (Mega-Kernel)The Problem: Heterogeneous components prevent batched processing and force interpreter intervention.The Solution: Group components by type and execute them in batches using a linearized dependency graph managed by an "Instruction Tape."Architecture:Component Grouping: Create arrays for each component type (e.g., ElectrolyzerState struct with arrays for efficiency, capacity, etc.).Instruction Tape: Create an integer array that dictates the flow execution.Example: ``The "Mega-Kernel": Write a single Numba @njit loop that runs the entire simulation timestep without returning to Python.Python@njit
def run_timestep(state_T, state_P, electrolyzer_params, instruction_tape):
    # This loop runs entirely in machine code
    for pc in range(len(instruction_tape)):
         opcode = instruction_tape[pc]
         # Dispatch to static handlers based on opcode
         if opcode == OP_ELECTROLYZER:
             update_electrolyzers(...)
         elif opcode == OP_COMPRESSOR:
             update_compressors(...)
Expected Gain:Zero Interpreter Overhead: The interpreter is invoked only once to start the simulation. The entire timestep loop runs in compiled machine code.Massive Speedup: Transitioning from Python dispatch to a compiled loop often yields 100x-1000x speedups.133.3.3 Static Flow Linearization with Integer ArraysThe Problem: execute_flows uses dictionary lookups.The Solution: Convert the flow network into integer adjacency arrays.upstream_indices: Array of size N_streams containing the index of the upstream component.downstream_indices: Array of size N_streams containing the index of the downstream component.execution_order: An integer array of component indices sorted topologically.The execute_flows logic becomes a simple integer loop over these arrays.Evidence: Snippet 15 and 16 confirm that array indexing is the fastest possible lookup method, superior to any hash map.4. Implementation Details & Risk Analysis4.1 Lookup Table Optimization ImplementationBilinear Interpolation Kernel:Instead of scipy.interpolate or generic implementations, use a specialized Numba kernel. If the grids are uniform, we can avoid the binary search entirely.Python@njit(fastmath=True)
def lookup_enthalpy(T, P, T_grid, P_grid, H_table):
    # Assumes T_grid and P_grid are uniform! 
    # This avoids searchsorted, becoming O(1) instead of O(log N)
    
    dT = T_grid - T_grid
    dP = P_grid - P_grid
    
    # Direct index calculation
    i = int((T - T_grid) / dT)
    j = int((P - P_grid) / dP)
    
    # Boundary handling omitted for brevity...
    
    # Bilinear formula here...
    return result
Optimization: By leveraging the uniformity of the grid, we reduce the complexity of finding the cell index from $O(\log N)$ to $O(1)$. This is a critical insight for performance in tight loops.124.2 Handling Monte Carlo & Long-Term RunsFor 8760h simulations with Monte Carlo analysis, the SoA approach enables trivial parallelization.Batch Dimension: Add a batch dimension to the State Matrix.Temperature[N_streams] becomes Temperature[N_streams, N_monte_carlo].Parallelization: Use numba.prange in the outer loop to parallelize across Monte Carlo scenarios.26 Since scenarios are independent, this is "embarrassingly parallel."Evidence: Snippet 26 shows that prange allows easy multi-threading, utilizing all CPU cores. By batching scenarios, we also improve cache locality, as the instructions for the physics models are reused for multiple data points.4.3 Accuracy Risk MitigationRisk: Utilizing float32 to save memory could violate the <0.1% error requirement.Analysis: Thermodynamic potentials (Enthalpy, Entropy) often have large absolute values. Small relative errors in T (Kelvin) or P (Pascal) can lead to significant errors in H.Mitigation:Strict Typing: Use float64 for all thermodynamic state variables (T, P, H, S). The memory bandwidth cost is justified by the stability and accuracy requirements.Validation Suite: Create a regression test suite comparing the SoA/Numba output against the original object-oriented/CoolProp implementation. Ensure deviation is strictly < 0.1%.Parameter Precision: Use float32 only for static parameters (e.g., pipe diameters) or visual outputs where precision is less critical, if memory becomes a hard constraint.5. Critical Conclusion: The Single Biggest BottleneckDespite the current usage of Numba for thermodynamic operations and LUTs for property retrieval, the Single Biggest Bottleneck remains the Memory Layout and Access Pattern defined by the Object-Oriented Architecture.Specifically:Pointer Chasing: The Stream objects scatter data across the heap, causing constant cache misses that stall the CPU.Interpreter Latency: The engine.py loop dispatching component.update() incurs Python overhead that dwarfs the optimized math happening inside the functions.Data Marshalling: Moving data between these Python objects and the Numba-optimized functions creates a "marshalling tax" that negates the benefits of JIT compilation.Final Recommendation:The simulator must undergo a Data-Oriented Transformation. You must stop thinking of the plant as a "collection of Component objects" and start treating it as a "System of State Arrays." By moving to a Structure of Arrays (SoA) layout and implementing Linearized Static Execution, you can keep the execution entirely within compiled machine code (Numba nopython mode) for the duration of the timestep loop. This approach effectively creates a "Simulation Virtual Machine" that runs at near-C speeds, enabling the required 8760h Monte Carlo throughput.Optimization LevelKey ActionPrimary GainComplexityLevel 1Uniform Grids & Loop UnrollingLocal computation speedup (20-30%)LowLevel 2Pre-computed Flow Lists (Linearization)O(1) Flow resolution (No Dicts)MediumLevel 3Structure of Arrays (SoA) & Mega-Kernel100x+ Simulation SpeedupHighRecommendation: Proceed immediately with Level 1 optimizations for quick wins, while prototyping the SoA "Compiler" (Level 3) for the core engine refactor. The move to SoA is not just an optimization; it is a prerequisite for the scale of simulation you intend to run.